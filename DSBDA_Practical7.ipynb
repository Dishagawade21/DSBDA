{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgQFfC0MrrKmqwM+WS/KKO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m01NirOCezNN","executionInfo":{"status":"ok","timestamp":1744697202803,"user_tz":-330,"elapsed":7342,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"197d3500-b03e-408b-df02-d4cc0844e152"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Error loading corpus: Package 'corpus' not found in index\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('corpus')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","import math\n"]},{"cell_type":"code","source":["text = \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences.\"\n"],"metadata":{"id":"tsw3O7p3fuMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","tokenized_text= sent_tokenize(text,language='english')\n","print(tokenized_text)\n","#Word Tokenization\n","from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text,language='english')\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQKCRAL5fkQ5","executionInfo":{"status":"ok","timestamp":1744436133899,"user_tz":-330,"elapsed":3,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"c87df954-6465-4b07-d342-3d454ecf3f41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences.']\n","['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', '.']\n"]}]},{"cell_type":"code","source":["import re\n","# print stop words of English\n","from nltk.corpus import stopwords\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)\n","text= \"How to remove stop words with NLTK library in Python?\"\n","text= re.sub('[^a-zA-Z]', ' ',text)\n","tokens = word_tokenize(text.lower())\n","filtered_text=[]\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"Tokenized Sentence:\",tokens)\n","print(\"Filterd Sentence:\",filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DFB5Q3LftZC","executionInfo":{"status":"ok","timestamp":1744436242646,"user_tz":-330,"elapsed":9,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"b9bf3d2e-b0e0-48c5-e38e-ada5a2e71583"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'here', 'about', 'wasn', \"we'll\", 'yours', 'hadn', 'aren', 'both', \"don't\", \"i'd\", 'yourself', 'been', 'itself', 'because', \"haven't\", 'to', 'who', 'weren', \"you're\", 'just', 'or', 'on', 'he', \"i'll\", 'it', \"hadn't\", 'our', 'as', 'has', \"you'll\", \"they'll\", \"we're\", 're', 'won', 'y', \"he's\", 'if', 'now', 'again', 'should', 'of', \"shouldn't\", 'more', 'most', 'too', 'during', 'from', \"they're\", 'an', 'shan', \"isn't\", 'when', 'myself', \"i'm\", \"she'd\", 'further', \"needn't\", 'you', 'them', \"won't\", 'him', \"i've\", 'whom', 's', 'themselves', \"he'll\", 'through', 'a', 'after', 'they', \"she's\", 'into', 'his', 'under', \"we've\", \"you'd\", 'can', 'while', \"doesn't\", 'were', 'o', \"you've\", \"we'd\", 'did', 'no', 'off', 'be', 'is', 'the', 'any', 'didn', \"it's\", 'for', \"weren't\", 'doesn', 'not', \"he'd\", 'there', 'mustn', 'these', \"they've\", \"they'd\", 'll', \"mightn't\", 'at', 'but', 'ain', 'until', \"wouldn't\", 'this', 'her', 'and', 'having', 'doing', 'yourselves', 'where', \"mustn't\", 'your', \"it'd\", 'its', 'have', 'me', 'are', \"should've\", 'above', 'those', 'himself', 'between', \"aren't\", 'herself', 'ma', 'being', \"that'll\", 'with', 'i', 'few', \"it'll\", 'do', 'hers', 'nor', 'needn', 'haven', 'ourselves', 'same', 'shouldn', 'than', 'such', 'then', 'd', 'mightn', 'm', 'against', 'some', 'down', 'what', 'was', \"she'll\", 'all', 'by', \"hasn't\", 'own', 'couldn', 'had', 'am', 'isn', \"wasn't\", 'before', 'in', 've', 'only', 'does', 'their', 'don', 'each', 'that', 'so', 'we', 't', 'wouldn', 'will', 'below', 'out', 'theirs', 'once', 'very', 'how', \"shan't\", 'why', \"didn't\", \"couldn't\", 'hasn', 'ours', 'she', 'which', 'over', 'up', 'other', 'my'}\n","Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n","Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","  rootWord=ps.stem(w)\n","print(rootWord)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-0cZoPUgOcI","executionInfo":{"status":"ok","timestamp":1744436266799,"user_tz":-330,"elapsed":3,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"496419ba-951c-4008-ca8a-7a3383c14fac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w,\n","wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iau_r6GrgTDr","executionInfo":{"status":"ok","timestamp":1744436287525,"user_tz":-330,"elapsed":3212,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"bfcbfe4e-bea5-47a0-c4e6-a28692a571a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvDKHbJ4gYOC","executionInfo":{"status":"ok","timestamp":1744436305124,"user_tz":-330,"elapsed":125,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"ab5fd97f-7d3d-470a-a6b8-449d6739bd09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"code","source":["#Algorithm for Create representation of document by calculating TFIDF"],"metadata":{"id":"js-XskDTgdka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"a6SPjYUxgbSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documentA = 'Jupiter is the largest Planet'\n","documentB = 'Mars is the fourth planet from the Sun'"],"metadata":{"id":"kXunlcrwghrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bagOfWordsA = documentA.split(' ')\n","bagOfWordsB = documentB.split(' ')"],"metadata":{"id":"_nTFf6EAgkXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example: Assuming bagOfWordsA and bagOfWordsB are lists of words\n","bagOfWordsA = ['this', 'is', 'a', 'sample', 'text']\n","bagOfWordsB = ['this', 'is', 'another', 'example']\n","uniqueWords = set(bagOfWordsA + bagOfWordsB)\n","numOfWordsA = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsA:\n","    numOfWordsA[word] += 1\n","numOfWordsB = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsB:\n","    numOfWordsB[word] += 1\n","print(\"Word counts in A:\", numOfWordsA)\n","print(\"Word counts in B:\", numOfWordsB)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BmIDImYgobD","executionInfo":{"status":"ok","timestamp":1744436422972,"user_tz":-330,"elapsed":4,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"e51a83de-5588-4fea-dff2-15a71be3c9b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word counts in A: {'sample': 1, 'a': 1, 'another': 0, 'this': 1, 'text': 1, 'is': 1, 'example': 0}\n","Word counts in B: {'sample': 0, 'a': 0, 'another': 1, 'this': 1, 'text': 0, 'is': 1, 'example': 1}\n"]}]},{"cell_type":"code","source":["def computeTF(wordDict, bagOfWords):\n","    tfDict = {}\n","    bagOfWordsCount = len(bagOfWords)\n","    for word, count in wordDict.items():\n","        tfDict[word] = count / float(bagOfWordsCount)\n","    return tfDict\n","bagOfWords = ['this', 'is', 'a', 'sample', 'text', 'text', 'this']\n","wordDict = {'this': 2, 'is': 1, 'a': 1, 'sample': 1, 'text': 2}\n","tf = computeTF(wordDict, bagOfWords)\n","print(\"Term Frequency:\", tf)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5jxePfLg4r4","executionInfo":{"status":"ok","timestamp":1744436473758,"user_tz":-330,"elapsed":4,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"c1e278fb-8fdb-4b0b-d56c-174deef96960"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Term Frequency: {'this': 0.2857142857142857, 'is': 0.14285714285714285, 'a': 0.14285714285714285, 'sample': 0.14285714285714285, 'text': 0.2857142857142857}\n"]}]},{"cell_type":"code","source":["import math\n","\n","def computeIDF(documents):\n","    unique_words = set(word for doc in documents for word in doc.keys())\n","\n","    N = len(documents)  # Number of documents\n","    idfDict = dict.fromkeys(unique_words, 0)  # Initialize IDF dictionary with all unique words\n","\n","    for document in documents:\n","        for word, val in document.items():\n","            if val > 0:\n","                idfDict[word] += 1  # Count how many documents contain each word\n","\n","    for word, val in idfDict.items():\n","        idfDict[word] = math.log(N / float(val))  # Compute the IDF for each word\n","    return idfDict\n","numOfWordsA = {'this': 2, 'is': 1, 'a': 1, 'sample': 1, 'text': 2}\n","numOfWordsB = {'this': 1, 'is': 1, 'another': 1, 'example': 1}\n","idfs = computeIDF([numOfWordsA, numOfWordsB])\n","print(\"IDF Values:\", idfs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJX6sxBIhKaj","executionInfo":{"status":"ok","timestamp":1744436600836,"user_tz":-330,"elapsed":5,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"e900f60a-2ad0-46b9-d2da-8bad15f7475c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IDF Values: {'sample': 0.6931471805599453, 'a': 0.6931471805599453, 'another': 0.6931471805599453, 'this': 0.0, 'text': 0.6931471805599453, 'is': 0.0, 'example': 0.6931471805599453}\n"]}]},{"cell_type":"code","source":["def computeTFIDF(tfBagOfWords, idfs):\n","    tfidf = {}\n","    for word, val in tfBagOfWords.items():\n","        tfidf[word] = val * idfs[word]  # Compute TF-IDF\n","    return tfidf  # Return the computed TF-IDF dictionary\n","tfA = {'this': 0.2857, 'is': 0.1429, 'a': 0.1429, 'sample': 0.1429, 'text': 0.2857}\n","tfB = {'this': 0.25, 'is': 0.25, 'another': 0.25, 'example': 0.25}\n","idfs = {'this': 0.4055, 'is': 0.0000, 'a': 0.4055, 'sample': 0.4055, 'text': 0.4055, 'another': 0.4055, 'example': 0.4055}\n","tfidfA = computeTFIDF(tfA, idfs)\n","tfidfB = computeTFIDF(tfB, idfs)\n","import pandas as pd\n","df = pd.DataFrame([tfidfA, tfidfB])\n","print(df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4jLG3l8hnMG","executionInfo":{"status":"ok","timestamp":1744436732993,"user_tz":-330,"elapsed":53,"user":{"displayName":"Disha Gawade","userId":"08413590762686857076"}},"outputId":"c4cc5614-441a-4ab9-9416-cb0d853e4f63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["       this   is         a    sample      text   another   example\n","0  0.115851  0.0  0.057946  0.057946  0.115851       NaN       NaN\n","1  0.101375  0.0       NaN       NaN       NaN  0.101375  0.101375\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Oh730r9QiGmh"},"execution_count":null,"outputs":[]}]}